Metadata-Version: 2.1
Name: unwanted-content-detector
Version: 0.1.0
Summary: A library to detect undesired, unbranded, or harmful content
License: Apache
Author: Jean Carlo Machado
Author-email: jean.machado@getyourguide.com
Requires-Python: >=3.10,<4.0
Classifier: License :: Other/Proprietary License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: fire (>=0.5.0,<0.6.0)
Requires-Dist: pandas (>=2.0.0,<3.0.0)
Requires-Dist: transformers (>=4.28.0,<5.0.0)
Description-Content-Type: text/markdown

# unwanted_content_detector

A library to detect undesired, unbranded, or harmful content

## Usage

In python:



```sh
pip install unwanted-content-detector

```

Minimal

```py
from unwanted_content_detector import Detector
detector = Detector(models=['hatefult_content_generic_distil_bert_finetuned'])
if detector.is_unwanted('content generated by llm'):
    print("Wont continue")
```

With spark
```py

spark_df.with_column('is_rejected', lambda row: detector.is_unwanted)
```

In the terminal

```sh
./cli.py inference infer 'text to be validated'
```


## Training 

Fine tunning

```py
from unwanted_content_detector import Detector
model = Detector({'data_source': df}).train()
```


```
./cli.py train
```


## Target Architecture / Features 

- multiple Swappable models
- multiple evaluation datasets
- possibility of configuring a custom personal dataset to fine tune
- Single performance evaluation criteria

## Use cases it could be applied to

- detecting the generation of harmful content from LLMs
- preventing harmful prompts to be injected into LLMs
- using it as a validator of content being generated according to the brand guidelines


## Liability

This tool aims to help you to detect harmful content but it is not meant to be used as the final decision maker alone. 

