Metadata-Version: 2.1
Name: langchain-llm-api
Version: 0.0.1
Summary: Wrappers around LLM API models and embeddings clients.
Home-page: https://github.com/1b5d/langchain-llm-api
License: MIT
Author: 1b5d
Author-email: 8110504+1b5d@users.noreply.github.com
Requires-Python: >=3.8.1,<4.0.0
Classifier: License :: OSI Approved :: MIT License
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Requires-Dist: langchain (>=0.0,<0.1)
Requires-Dist: pydantic (>=1.10,<2.0)
Requires-Dist: requests (>=2.28,<3.0)
Requires-Dist: sseclient-py (>=1.7,<2.0)
Project-URL: Repository, https://github.com/1b5d/langchain-llm-api
Description-Content-Type: text/markdown

# Langchain LLM API

A Langchain compatible implementation which enables the integration with [LLM-API](https://github.com/1b5d/llm-api) 

The main reason for implementing this package is to be able to use Langchain with any model run locally.

# Usage

You can install this as a python library using the command (until it's integrated with langchain itself)

```
pip install langchain-llm-api
```

To use this langchain implementation with the LLM-API:

```
from langchain_llm_api import LLMAPI, APIEmbeddings

llm = LLMAPI(
    params={"temp": 0.2},
    verbose=True
)

llm("What is the capital of France?")

```

Or with streaming:

```
from langchain_llm_api import LLMAPI, APIEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler

llm = LLMAPI(
    params={"temp": 0.2},
    verbose=True,
    streaming=True,
    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])
)

llm("What is the capital of France?")

```

Check [LLM-API](https://github.com/1b5d/llm-api) for the possible models and thier params

## Embeddings

to use the embeddings endpoint:

```
emb = APIEmbeddings(
    host_name="your api host name",
    params = {"n_predict": 300, "temp": 0.2, ...}
)
```

